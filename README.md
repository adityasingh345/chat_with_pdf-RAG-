ğŸ“„ Chat with PDF using RAG (FAISS + Ollama + FastAPI + React)

An end-to-end Retrieval-Augmented Generation (RAG) application that allows users to upload a PDF and ask questions about its content.

The system performs semantic search over the document using FAISS and generates context-grounded answers using a local LLM (Ollama).

âš ï¸ Built without LangChain to deeply understand how RAG works under the hood.

ğŸš€ Features

ğŸ“¤ Upload any PDF document

âœ‚ï¸ Automatic text extraction & chunking

ğŸ§  Semantic embeddings using Sentence Transformers

ğŸ” Fast similarity search with FAISS

ğŸ¤– Context-aware answers via Ollama (local LLM)

ğŸ§© Backend API built with FastAPI

ğŸ’» Simple React frontend for interaction

ğŸ” Document isolation using doc_id

ğŸ—ï¸ Architecture Overview
User (Browser)
   â†“
React Frontend
   â†“ HTTP
FastAPI Backend
   â†“
PDF â†’ Chunks â†’ Embeddings â†’ FAISS
   â†“
Question â†’ FAISS Search â†’ Context
   â†“
Prompt â†’ Ollama (LLM)
   â†“
Answer â†’ Frontend

ğŸ§  How RAG Works in This Project

PDF Upload

User uploads a PDF from the frontend

Backend extracts text and splits it into overlapping chunks

Embedding & Indexing

Each chunk is converted into a vector embedding

Vectors are stored in a FAISS index (per document)

Document Identification

Each uploaded PDF gets a unique doc_id

FAISS index and chunks are stored under this doc_id

Question Answering

User asks a question with the corresponding doc_id

Backend retrieves top-k relevant chunks

LLM answers using only the retrieved context

ğŸ› ï¸ Tech Stack
Backend

FastAPI â€“ REST API

FAISS â€“ Vector similarity search

Sentence-Transformers â€“ Text embeddings

Ollama â€“ Local LLM inference

PyPDF â€“ PDF text extraction

Frontend

React (Vite) â€“ UI

Fetch API â€“ Backend communication

ğŸ“ Project Structure
rag-chat-with-pdf/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py            # FastAPI routes
â”‚   â”œâ”€â”€ ingest.py         # PDF ingestion logic
â”‚   â”œâ”€â”€ rag.py            # RAG pipeline
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ uploads/
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ api.js
â”‚   â”‚   â””â”€â”€ main.jsx
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md

âš™ï¸ Setup Instructions
1ï¸âƒ£ Prerequisites

Python 3.9+

Node.js 18+

Ollama installed
ğŸ‘‰ https://ollama.com

2ï¸âƒ£ Start Ollama
ollama pull llama3.1
ollama run llama3.1


(Ollama runs at http://localhost:11434)

3ï¸âƒ£ Backend Setup
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn app:app --reload


Backend will run at:

http://127.0.0.1:8000


Swagger UI:

http://127.0.0.1:8000/docs

4ï¸âƒ£ Frontend Setup
cd frontend
npm install
npm run dev


Frontend runs at:

http://localhost:5173

ğŸ§ª How to Use the App

Upload a PDF

Backend returns a doc_id

Ask questions related to that PDF

Receive context-grounded answers

Example questions:

What does the document say about physical wellness?

Summarize the main topics discussed

What are the authorâ€™s reading interests?

ğŸ” Why doc_id?

Each PDF is assigned a unique doc_id to:

Prevent mixing content from different documents

Support multiple uploads

Scope all queries to the correct FAISS index

âš ï¸ Current Limitations

FAISS indexes are stored in memory

Restarting backend clears uploaded documents

No authentication

One document per doc_id

Ollama makes deployment to free platforms difficult

These are intentional trade-offs for learning clarity.

ğŸš€ Future Improvements

Persist FAISS indexes to disk

Support multiple PDFs per session

Streaming responses to frontend

Add citations (page numbers)

Switchable LLM backend (OpenAI / HF)
